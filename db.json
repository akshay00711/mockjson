{
  "posts": [
    { "id": 1, "title": "Post 1" },
    { "id": 2, "title": "Post 2" },
    { "id": 3, "title": "Post 3" }
  ],
  "comments": [
    { "id": 1, "body": "some comment", "postId": 1 },
    { "id": 2, "body": "some comment", "postId": 1 }
  ],
  "profile": {
    "name": "typicode"
  },
  "allPost" : [
    {
      "slug": "sora-ai",
      "title": "Creating video from text",
      "date": "2023-10-16",
      "image": "sora-ai.png",
      "excerpt": "Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user’s prompt.",
      "isFeatured": false,
      "readerCount": "1.2k",
      "content": "\r\n Sora is an AI model that can create realistic and imaginative scenes from text instructions.\r\n \r\n Today, Sora is becoming available to red teamers to assess critical areas for harms or risks.\r\n  \r\n We are also granting access to a number of visual artists, designers, and filmmakers to gain feedback on how to advance the model to be most helpful for creative professionals.\r\n \r\n We’re sharing our research progress early to start working with and getting feedback from people outside of OpenAI and to give the public a sense of what AI capabilities are on the horizon.\r\n \r\n ## Safety\r\n \r\n  We’ll be taking several important safety steps ahead of making Sora available in OpenAI’s products.\r\n \r\n"
    }{
  "slug": "ALBERT",
  "title": "A Lite BERT",
  "excerpt": "A lite version of BERT for efficient training and inference developed by Google Research Team",
  "image": "ALBERT.png",
  "isFeatured": false,
  "readerCount": "1.5k",
  "date": "2022-12-30",
 "content": "\r\n         •	Model Description: A lite version of BERT for efficient training and inference.         •	Model Category: Natural Language Processing (NLP)         •	Model Developer/Organization: Google Research         •	Model Architecture: Transformer         •	Key Features: Reduced model size and computational cost while maintaining performance.         •	Performance Metrics: Competitive performance compared to BERT with reduced parameters.         •	Availability: Open-source (licensed under Apache License 2.0)         Geeksforgeeks: https://paperswithcode.com/paper/albert-a-lite-bert-for-self-supervised         Google research blog: http://blog.research.google/2019/12/albert-lite-bert-for-self-supervised.html         \r\n \r\n"
},
{
  "slug": "EfficientNet",
  "title": "Scalable and efficient Convolutional Neural Network (CNN) architecture.",
  "excerpt": "Computer Vision Model developed by Google Research Team",
  "image": "EfficientNet.png",
  "isFeatured": true,
  "readerCount": "2.5k",
  "date": "2022-12-30",
  "content": "\r\n         •	Model Description: Scalable and efficient Convolutional Neural Network (CNN) architecture.         •	Model Category: Computer Vision         •	Model Developer/Organization: Google Research         •	Model Architecture: Convolutional Neural Network (CNN)         •	Key Features: Achieves state-of-the-art performance with fewer parameters and computations.         •	Performance Metrics: Achieved state-of-the-art results on ImageNet classification tasks.         •	Availability: Open-source (licensed under Apache License 2.0)         Google research blog: https://blog.research.google/2019/05/efficientnet-improving-accuracy-and.html?m=1         Github: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/README.md         \r\n \r\n"
},
{
  "slug": "T5",
  "title": "Text-To-Text Transfer Transformer",
  "excerpt": "General-purpose text-to-text transformer model developed by Google AI Language Team",
  "image": "T5.png",
  "isFeatured": false,
  "readerCount": "1.7k",
  "date": "2022-12-30",
  "content": "\r\n         •	Model Description: General-purpose text-to-text transformer model.         •	Model Developer/Organization: Google AI Language Team         •	Model Architecture: Transformer         •	Key Features: Unified framework for various NLP tasks by framing them as text-to-text transformations.         •	Pe•	Model Category: Natural Language Processing (NLP)         rformance Metrics: Achieved state-of-the-art results in several NLP benchmarks.         •	Availability: Open-source (licensed under Apache License 2.0)         Github blog: https://github.com/google-research/text-to-text-transfer-transformer         Google research blog: http://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html         \r\n \r\n"
},
  ]
}
